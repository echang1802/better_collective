{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Collective Challenge - Predictive Modeling\n",
    "\n",
    "Eloy Chang\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from scipy.stats import randint, uniform\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/train_data.pkl', 'rb') as file:\n",
    "    X_train, y_train = pickle.load(file)\n",
    "\n",
    "with open('../data/test_data.pkl', 'rb') as file:\n",
    "    X_test, y_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "As a baseline model a logistic regression will be used, this is a good model for this because is a very simple model, easy to explain and this model can provide interesting insights, however, this model may not be highly accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eloyc\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7425886712546321,\n",
       " 'precision': 0.9744763245907411,\n",
       " 'recall': 0.7546346782988005,\n",
       " 'f1-score': 0.8505800107551664,\n",
       " 'roc-auc': 0.5290686651135125}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training\n",
    "baseline_model = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "# Metrics\n",
    "baseline_predictions = baseline_model.predict(X_test)\n",
    "baseline_metrics = {\n",
    "    'accuracy' : baseline_model.score(X_test, y_test),\n",
    "    'precision' : precision_score(y_test, baseline_predictions),\n",
    "    'recall' : recall_score(y_test, baseline_predictions),\n",
    "    'f1-score' : f1_score(y_test, baseline_predictions),\n",
    "    'roc-auc' : roc_auc_score(y_test, baseline_model.predict_proba(X_test)[:, 1])\n",
    "}\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has high values of accuracy, precision and recall, but this metrics are tricky because the unbalance of the data; the AUC-ROC is a good metrics in this case, but the results for this model is very low. \n",
    "\n",
    "## Churn Model\n",
    "\n",
    "The ensamble models are very powerfull tree based model, one of them is the Gradient Boosting algorithm. These model have several parameters that we can optimice, in order to improve the model performance, to do so, the Randomized Search Cross Validation will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters: {'learning_rate': 0.6976311959272649, 'max_depth': 43, 'min_samples_leaf': 0.4849045338733744}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7204870301746956,\n",
       " 'precision': 0.9699532205829435,\n",
       " 'recall': 0.7348691384950927,\n",
       " 'f1-score': 0.8362028850628199,\n",
       " 'roc-auc': 0.5071995514027957}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = y_train['churn'].copy()\n",
    "weights.loc[weights == 0] = 6\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "random_forest_model = GradientBoostingClassifier(n_estimators = 500, random_state=0)\n",
    "distributions = {\n",
    "    'learning_rate' : uniform(loc=0, scale=1),\n",
    "    'max_depth' : randint(2, 50),\n",
    "    'min_samples_leaf' : uniform(loc=0, scale=.5)\n",
    "}\n",
    "\n",
    "model = RandomizedSearchCV(random_forest_model, distributions, random_state=0, n_iter = 20, cv = 5)\n",
    "best_model = model.fit(X_train, np.array(y_train)[:, 0], sample_weight = weights)\n",
    "\n",
    "print(f'Best model parameters: {best_model.best_params_}')\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "model_metrics = {\n",
    "    'accuracy' : model.score(X_test, y_test),\n",
    "    'precision' : precision_score(y_test, predictions),\n",
    "    'recall' : recall_score(y_test, predictions),\n",
    "    'f1-score' : f1_score(y_test, predictions),\n",
    "    'roc-auc' : roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
    "}\n",
    "model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model/model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Model evaluation\n",
    "\n",
    "#### Effectiveness \n",
    "\n",
    "Machine learning models lose prediction power over time due to change on the behaviour of variables, this is why there should be a scheduled process to, from time to time, measure how well is performing the model with a selected metric like AUC - ROC, once the model performance go below a defined threshold a alert can be raise, it is also possible to automatically run a re train process using the same methodology but with newest data, if the model performance do not improve as wanted a new model can be manually trained using new variables as well.\n",
    "\n",
    "#### summary\n",
    "\n",
    "* 88% of the accounts churn the same month they register, however, these accounts represent the 62.9% of the total revenue; although is not ideal the short life of these account, they still generate a great part of the revenue, but, on the other side, this generate  a data issue given the huge unbalance of the data. \n",
    "\n",
    "* Most of the variables have a high variability, a segmentation prior to the model is highly recommendable to reduce noice on the model.\n",
    "\n",
    "* There are some brands and ads locations that seems to generate huge losses.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "#### Aditional Data\n",
    "\n",
    "* Demographic data.\n",
    "\n",
    "* Lead history on the sites.\n",
    "\n",
    "* Account history between brands.\n",
    "\n",
    "#### Metrics\n",
    "\n",
    "I think the most valuable metrics is the revenue generated by the account, however, the model should be based on the percentage of that revenue that goes to Better Collective, given that, may not be the same maximize the revenue of the partners that maximize the revenue of Better Collective.\n",
    "\n",
    "\n",
    "#### Ideas\n",
    "\n",
    "* I would start obtaining more data and improving the data cleaning process; a high amount of data was dropped during the analysis, drill down on the root causes of these inconsistent and / or generate rules or models to correctly impute them is highly recommendable.\n",
    "\n",
    "* Train a clusterization algorithm to reduce variance and noise on the variables, this not only may improve models performances, but also may generate interesting insights. \n",
    "\n",
    "* Train a model to estimate the revenue generated by the account each month, together with this model a prediction of the CLV is duable. \n",
    "\n",
    "* Once a model to predict the CLV is completed with a acceptable performance, this can be used to optimize several other process as:\n",
    "\n",
    "    * Ads selection and ubications. \n",
    "\n",
    "    * Improve negotiation with partners. \n",
    "\n",
    "    * Improve customer experience. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
